{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Libraries](#libraries)\n",
    "2. [Numerical columns](#numerical)\n",
    "3. [Categorical columns](#categorical)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id = \"libraries\"> 1. Libraries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from scipy.stats import sem\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../Data/retyped_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = \"numerical\"> <h1>Evalutation </h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Section is dedicated to evaluate imputation using similarity strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Function to fill missing using similarity strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "start = 0\n",
    "end = 32\n",
    "\n",
    "# Define the function to calculate similarities\n",
    "def calculate_similarities(ratings, batch_start, batch_end):\n",
    "    # Select the batch of users\n",
    "    batch_ratings = ratings[batch_start:batch_end]\n",
    "    \n",
    "    # Calculate the absolute difference between the batch and all users\n",
    "    abs_diff = np.abs(ratings - batch_ratings.reshape(batch_end - batch_start, 1, ratings.shape[1]))\n",
    "    \n",
    "    # Calculate the mean absolute difference across movies, ignoring NaN values\n",
    "    mean_diff = np.nanmean(abs_diff, axis=2)\n",
    "    \n",
    "    # Compute similarity as the inverse of the mean absolute difference\n",
    "    similarities = 1 / (mean_diff + 0.001)  # Adding a small epsilon to avoid division by zero\n",
    "    similarities[np.isnan(similarities)] = 0\n",
    "    return similarities\n",
    "\n",
    "def fill_missing(data, batch_size = 32):\n",
    "    n_movies = data.shape[0]\n",
    "    filled_ratings = np.empty_like(data)\n",
    "    num_batches = int(np.ceil(n_movies / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, n_movies)\n",
    "\n",
    "        similarities = calculate_similarities(data, start, end)\n",
    "        \n",
    "        weights = ~np.isnan(data) * similarities.reshape(end - start, -1, 1)\n",
    "        weights /= weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "        filled_ratings[start:end] = np.nansum(data * weights, axis=1)\n",
    "\n",
    "    return filled_ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Evaluate each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sumary of Tomatoes CriticScore: \n",
      "Mean absolute error:  19.754701330716717\n",
      "Mean squared error:  525.6943104026549\n",
      "R2 score:  0.31772289189241265\n",
      "--------\n",
      "\n",
      "Sumary of Tomatoes UserScore: \n",
      "Mean absolute error:  14.239382392822442\n",
      "Mean squared error:  289.0440282119497\n",
      "R2 score:  0.3071021599944561\n",
      "--------\n",
      "\n",
      "Sumary of Metascore: \n",
      "Mean absolute error:  12.497650376541852\n",
      "Mean squared error:  232.4710433204248\n",
      "R2 score:  0.3416715140083487\n",
      "--------\n",
      "\n",
      "Sumary of Meta UserScore: \n",
      "Mean absolute error:  0.8212265709625988\n",
      "Mean squared error:  1.084668638219071\n",
      "R2 score:  0.29630987159548206\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "\n",
    "test_size = 0.3\n",
    "\n",
    "#Evalue each column seperatly\n",
    "for test_col in columns:\n",
    "    #Get a copy of data but remove all null value for testing\n",
    "    raw_data_copy = raw_data.copy()\n",
    "    raw_data_copy.dropna(inplace=True)\n",
    "    raw_data_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Get sample with size of 30%\n",
    "    test_rows = raw_data_copy.sample(frac=test_size, random_state=42).index\n",
    "\n",
    "    #Get y true value from the dataset\n",
    "    y_test = raw_data_copy.loc[test_rows, test_col].copy()\n",
    "\n",
    "    #Assign it's as nan value for imputing\n",
    "    raw_data_copy.loc[test_rows, test_col] = np.nan\n",
    "\n",
    "    #Perform imputing missing value using similarity\n",
    "    tmp_data = raw_data_copy.copy()\n",
    "    tmp_data['id'] = tmp_data.index\n",
    "    tmp_data['Meta UserScore'] = tmp_data['Meta UserScore'] * 10\n",
    "\n",
    "    tmp_data = tmp_data[['id','Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']].to_numpy()\n",
    "\n",
    "    filled_ratings = fill_missing(tmp_data)\n",
    "    filled_nanvals = filled_ratings[np.isnan(tmp_data)]\n",
    "\n",
    "    tmp_data[np.isnan(tmp_data)] = filled_nanvals\n",
    "\n",
    "    filled_df = pd.DataFrame(\n",
    "        filled_ratings[:, 1:],\n",
    "        columns=['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "    )\n",
    "\n",
    "    filled_df['Meta UserScore'] /= 10\n",
    "    tmp_data_2 = raw_data_copy.copy()\n",
    "\n",
    "    for col in filled_df.columns:\n",
    "        tmp_data_2[col].fillna(filled_df[col], inplace=True)\n",
    "\n",
    "    #Get y predicted (column after imputing)\n",
    "    y_pred = tmp_data_2.loc[test_rows, test_col]\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Sumary of {test_col}: ')\n",
    "    print('Mean absolute error: ', mae)\n",
    "    print('Mean squared error: ', mse)\n",
    "    print('R2 score: ', r2)\n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Loss seem too high, suggest using other imputation strategy (KNN, Decision Tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
