{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Libraries](#libraries)\n",
    "2. [Numerical columns](#numerical)\n",
    "3. [Categorical columns](#categorical)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id = \"libraries\"> 1. Libraries </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from scipy.stats import sem\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../Data/retyped_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id = \"numerical\"> <h1>Evalutation </h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Section is dedicated to evaluate imputation using similarity strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Function to fill missing using similarity strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 32\n",
    "start = 0\n",
    "end = 32\n",
    "\n",
    "# Define the function to calculate similarities\n",
    "def calculate_similarities(ratings, batch_start, batch_end):\n",
    "    # Select the batch of users\n",
    "    batch_ratings = ratings[batch_start:batch_end]\n",
    "    \n",
    "    # Calculate the absolute difference between the batch and all users\n",
    "    abs_diff = np.abs(ratings - batch_ratings.reshape(batch_end - batch_start, 1, ratings.shape[1]))\n",
    "    \n",
    "    # Calculate the mean absolute difference across movies, ignoring NaN values\n",
    "    mean_diff = np.nanmean(abs_diff, axis=2)\n",
    "    \n",
    "    # Compute similarity as the inverse of the mean absolute difference\n",
    "    similarities = 1 / (mean_diff + 0.001)  # Adding a small epsilon to avoid division by zero\n",
    "    similarities[np.isnan(similarities)] = 0\n",
    "    return similarities\n",
    "\n",
    "def fill_missing(data, batch_size = 32):\n",
    "    n_movies = data.shape[0]\n",
    "    filled_ratings = np.empty_like(data)\n",
    "    num_batches = int(np.ceil(n_movies / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, n_movies)\n",
    "\n",
    "        similarities = calculate_similarities(data, start, end)\n",
    "        \n",
    "        weights = ~np.isnan(data) * similarities.reshape(end - start, -1, 1)\n",
    "        weights /= weights.sum(axis=1, keepdims=True)\n",
    "\n",
    "        filled_ratings[start:end] = np.nansum(data * weights, axis=1)\n",
    "\n",
    "    return filled_ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Evaluate each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sumary of Tomatoes CriticScore: \n",
      "Mean absolute error:  19.754701330716717\n",
      "Mean squared error:  525.6943104026549\n",
      "R2 score:  0.31772289189241265\n",
      "--------\n",
      "\n",
      "Sumary of Tomatoes UserScore: \n",
      "Mean absolute error:  14.239382392822442\n",
      "Mean squared error:  289.0440282119497\n",
      "R2 score:  0.3071021599944561\n",
      "--------\n",
      "\n",
      "Sumary of Metascore: \n",
      "Mean absolute error:  12.497650376541852\n",
      "Mean squared error:  232.4710433204248\n",
      "R2 score:  0.3416715140083487\n",
      "--------\n",
      "\n",
      "Sumary of Meta UserScore: \n",
      "Mean absolute error:  0.8212265709625988\n",
      "Mean squared error:  1.084668638219071\n",
      "R2 score:  0.29630987159548206\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "\n",
    "test_size = 0.3\n",
    "\n",
    "#Evalue each column seperatly\n",
    "for test_col in columns:\n",
    "    #Get a copy of data but remove all null value for testing\n",
    "    raw_data_copy = raw_data.copy()\n",
    "    raw_data_copy.dropna(inplace=True)\n",
    "    raw_data_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Get sample with size of 30%\n",
    "    test_rows = raw_data_copy.sample(frac=test_size, random_state=42).index\n",
    "\n",
    "    #Get y true value from the dataset\n",
    "    y_test = raw_data_copy.loc[test_rows, test_col].copy()\n",
    "\n",
    "    #Assign it's as nan value for imputing\n",
    "    raw_data_copy.loc[test_rows, test_col] = np.nan\n",
    "\n",
    "    #Perform imputing missing value using similarity\n",
    "    tmp_data = raw_data_copy.copy()\n",
    "    tmp_data['id'] = tmp_data.index\n",
    "    tmp_data['Meta UserScore'] = tmp_data['Meta UserScore'] * 10\n",
    "\n",
    "    tmp_data = tmp_data[['id','Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']].to_numpy()\n",
    "\n",
    "    filled_ratings = fill_missing(tmp_data)\n",
    "    filled_nanvals = filled_ratings[np.isnan(tmp_data)]\n",
    "\n",
    "    tmp_data[np.isnan(tmp_data)] = filled_nanvals\n",
    "\n",
    "    filled_df = pd.DataFrame(\n",
    "        filled_ratings[:, 1:],\n",
    "        columns=['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "    )\n",
    "\n",
    "    filled_df['Meta UserScore'] /= 10\n",
    "    tmp_data_2 = raw_data_copy.copy()\n",
    "\n",
    "    for col in filled_df.columns:\n",
    "        tmp_data_2[col].fillna(filled_df[col], inplace=True)\n",
    "\n",
    "    #Get y predicted (column after imputing)\n",
    "    y_pred = tmp_data_2.loc[test_rows, test_col]\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Sumary of {test_col}: ')\n",
    "    print('Mean absolute error: ', mae)\n",
    "    print('Mean squared error: ', mse)\n",
    "    print('R2 score: ', r2)\n",
    "    print(\"--------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Loss seem too high, suggest using other imputation strategy (KNN, Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Section is dedicated to evaluate imputation using KNN strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Tomatoes CriticScore:\n",
      "Mean Absolute Error: 7.434468937875752\n",
      "Mean Squared Error: 102.7994488977956\n",
      "R² Score: 0.8665808069040736\n",
      "--------\n",
      "\n",
      "Summary of Tomatoes UserScore:\n",
      "Mean Absolute Error: 10.939478957915831\n",
      "Mean Squared Error: 188.24716933867734\n",
      "R² Score: 0.548732911630053\n",
      "--------\n",
      "\n",
      "Summary of Metascore:\n",
      "Mean Absolute Error: 5.980160320641283\n",
      "Mean Squared Error: 61.349383767535066\n",
      "R² Score: 0.8262663325490669\n",
      "--------\n",
      "\n",
      "Summary of Meta UserScore:\n",
      "Mean Absolute Error: 0.5763476953907815\n",
      "Mean Squared Error: 0.6138563877755512\n",
      "R² Score: 0.60175424538414\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the data (numeric columns only)\n",
    "columns = ['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "test_size = 0.3\n",
    "\n",
    "for test_col in columns:\n",
    "    # Step 1: Prepare the test dataset\n",
    "    raw_data_copy = raw_data.copy()\n",
    "    raw_data_copy.dropna(inplace=True)\n",
    "    raw_data_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Select test rows (30% of the data)\n",
    "    test_rows = raw_data_copy.sample(frac=test_size, random_state=42).index\n",
    "    y_test = raw_data_copy.loc[test_rows, test_col].copy()\n",
    "\n",
    "    # Mask test column values (set them to NaN for imputation)\n",
    "    raw_data_copy.loc[test_rows, test_col] = np.nan\n",
    "\n",
    "    # Step 2: Apply KNN Imputation\n",
    "    knn_imputer = KNNImputer(n_neighbors=20, weights=\"uniform\")\n",
    "    imputed_data = knn_imputer.fit_transform(raw_data_copy[columns])\n",
    "\n",
    "    # Reconstruct the imputed DataFrame\n",
    "    imputed_df = pd.DataFrame(imputed_data, columns=columns)\n",
    "\n",
    "    # Step 3: Evaluate the imputed values\n",
    "    y_pred = imputed_df.loc[test_rows, test_col]\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Summary of {test_col}:')\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    print('R² Score:', r2)\n",
    "    print('--------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss seemed fairly low, good stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Section is dedicated to evaluate imputation using Decision Tree strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Tomatoes CriticScore:\n",
      "Mean Absolute Error: 7.835627985849604\n",
      "Mean Squared Error: 112.20334479145103\n",
      "R² Score: 0.8545571626458768\n",
      "--------\n",
      "\n",
      "Summary of Tomatoes UserScore:\n",
      "Mean Absolute Error: 9.902754603179579\n",
      "Mean Squared Error: 171.0411980186958\n",
      "R² Score: 0.5896257803637216\n",
      "--------\n",
      "\n",
      "Summary of Metascore:\n",
      "Mean Absolute Error: 6.14279581338172\n",
      "Mean Squared Error: 68.55870330237295\n",
      "R² Score: 0.8058853347446275\n",
      "--------\n",
      "\n",
      "Summary of Meta UserScore:\n",
      "Mean Absolute Error: 0.6125306144772517\n",
      "Mean Squared Error: 0.6859065033066831\n",
      "R² Score: 0.5545997041376811\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the data (numeric columns only)\n",
    "columns = ['Tomatoes CriticScore', 'Tomatoes UserScore', 'Metascore', 'Meta UserScore']\n",
    "test_size = 0.3\n",
    "\n",
    "for test_col in columns:\n",
    "    # Step 1: Prepare the test dataset\n",
    "    raw_data_copy = raw_data.copy()\n",
    "    raw_data_copy.dropna(inplace=True)\n",
    "    raw_data_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Train Test split\n",
    "    train_col = columns.copy()\n",
    "    train_col.remove(test_col)\n",
    "\n",
    "    X = raw_data_copy[train_col].values\n",
    "    y = raw_data_copy[test_col].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    #Train model\n",
    "    decision_tree = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "        \n",
    "    #Evaluate\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Summary of {test_col}:')\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    print('R² Score:', r2)\n",
    "    print('--------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss seemed fairly low, good stuff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
